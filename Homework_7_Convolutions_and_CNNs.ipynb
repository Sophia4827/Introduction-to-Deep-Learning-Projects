{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Convolutions and CNNs\n",
        "\n",
        "## Problems\n",
        "\n",
        "1. Implementing Convolutions\n",
        "2. Interpreting Convolutions\n",
        "3. CIFAR-10 CNN\n",
        "4. Subword Tokenizer\n"
      ],
      "metadata": {
        "id": "tyNJ0sB9eYaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_QRqumeWgm"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Implementing Convolutions\n",
        "\n",
        "Implement the function `conv()` below, which takes in two arguments:\n",
        "* `data` is a one-dimensional array that contains the data to convolve.\n",
        "* `filter` is a one-dimensional array that contains the filter. This array is always an odd length.\n",
        "\n",
        "And returns the convolution of the two. Some additional notes:\n",
        "* You do not need to reverse the filter. We won't worry about that.\n",
        "* Make sure to pad your input array so that the output is the same length as the input!\n",
        "\n",
        "*You should not use any imported libraries, beyond basic array operations from numpy. The goal of this problem is to implement convolutions from basic coding.*"
      ],
      "metadata": {
        "id": "M_lnIg8zgcsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(data, filter):\n",
        "  data_len = len(data)\n",
        "  filter_len = len(filter)\n",
        "\n",
        "  pad_size = filter_len // 2\n",
        "  pad_data = [0] * pad_size + data + [0] * pad_size\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for i in range(data_len):\n",
        "    window = pad_data[i:i + filter_len]\n",
        "    value = sum(window[j] * filter[j] for j in range(filter_len))\n",
        "\n",
        "    result.append(value)\n",
        "  return result"
      ],
      "metadata": {
        "id": "PC_6HCiYb-LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Here are some test cases for your code"
      ],
      "metadata": {
        "id": "zexeWhsHyljO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(conv([0,1,3,4,2,1], [0,1,0]))      # should output [0, 1, 3, 4, 2, 1]\n",
        "print(conv([1,2,3,4,5,6,7,8], [-1,1,0])) # [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "print(conv([0,1,2,0,4,2,1], [-1,1,0]))   # [0, 1, 1, -2, 4, -2, -1]\n",
        "print(conv([0,1,0,4,3,0,2], [-1,3,-1]))  # [-1, 3, -5, 9, 5, -5, 6]"
      ],
      "metadata": {
        "id": "oG3F6WUi0yXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047dc90f-2863-41d2-fa17-99dfb285a024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 3, 4, 2, 1]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 1, 1, -2, 4, -2, -1]\n",
            "[-1, 3, -5, 9, 5, -5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Here are more test cases, but the filters are allowed to be any odd-size, not just 3 entries. You should only have one function that completes all of the test cases for a and b."
      ],
      "metadata": {
        "id": "J7a93vM6ygTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(conv([0,1,3,4,2,1], [0,0,1,0,0]))     # should output [0, 1, 3, 4, 2, 1]\n",
        "print(conv([1,2,3,4,5,6,7,8], [1,1,1,1,1])) # [6, 10, 15, 20, 25, 30, 26, 21]\n",
        "print(conv([0,1,2,0,4,2,1], [-1,1,-1]))     # [-1, -1, 1, -6, 2, -3, -1]\n",
        "print(conv([0,1,2,3,4,5,6], [0,1,2,3,4,5,6]))  # [32, 50, 70, 91, 70, 50, 32]"
      ],
      "metadata": {
        "id": "4NE7whun0nVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4d097a-f4cd-4065-e273-df4a86ee4032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 3, 4, 2, 1]\n",
            "[6, 10, 15, 20, 25, 30, 26, 21]\n",
            "[-1, -1, 1, -6, 2, -3, -1]\n",
            "[32, 50, 70, 91, 70, 50, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Interpreting Convolutions\n",
        "\n",
        "Consider the following data and the two filters. For this problem, assume that the datasets have an arbitrary number of 0s at either end, to avoid the edge effects of convolutions."
      ],
      "metadata": {
        "id": "vLqgsLJk1992"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [0,1,3,4,2,1,0]\n",
        "filter1 = [0.3333, 0.3333, 0.3333]\n",
        "filter2 = [0, -1, 1]"
      ],
      "metadata": {
        "id": "HRbE6wXI-Ga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter3 = [0.25, 0.5, 0.25]\n",
        "z = np.convolve(data, filter3, mode=\"same\")\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zazQUVM3sw7Q",
        "outputId": "27ff32ba-525d-4a1c-b74c-2a076d909ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 1.25, 2.75, 3.25, 2.25, 1.  , 0.25])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Notice that when we convolve `data` with `filter1`, the original data and the convolution have the same sum (up to rounding errors).\n",
        "* Will this be the case for all possible lists of data? Explain.\n",
        "* Describe what kind of filters will produce this result. Explain."
      ],
      "metadata": {
        "id": "-AGhpmK79LEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.convolve(data, filter2, mode=\"same\")\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0iXUV75wc0B",
        "outputId": "d7dcf87b-91ef-4737-e781-1d4ba591a70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, -1, -2, -1,  2,  1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(data))\n",
        "print(np.sum(conv(data, filter1)))\n",
        "\n",
        "# Yes, this will be the case for all possible lists of data because this filter is\n",
        "# a filter used for averaging the data. It looks at the middle and averages the things\n",
        "# around it. Because all the numbers in the filter sum up to 1, the sum of the\n",
        "# orginal data and the sum of the convolved data will be the same.\n",
        "\n",
        "# The kind of filters that will produce this are filters that sum up to 1 or any filter\n",
        "# that smoothes the data. An example of this would be [0.25, 0.5, 0.25] because it\n",
        "# smoothes the data set. If we used this data with the [0.25, 0.5, 0.25] filter, our\n",
        "# result will be [0.25, 1.25, 2.75, 3.25, 2.25, 1.0, 0.25], which gives us a sum of 1.\n",
        "# Like filter1, it looks at the middle number and averages the numbers to the left and\n",
        "# right of that number."
      ],
      "metadata": {
        "id": "UjdpG-AG4zt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d7f0e3-ca91-4791-dac7-ddcdaf92e13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "10.998899999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) When we convolve `data` with `filter2`, the convolution adds up to 0.\n",
        "* Will this be the case for all possible lists of data? Explain.\n",
        "* Describe what kind of filters will produce this result. Explain."
      ],
      "metadata": {
        "id": "si7kNUmV5mn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(data))\n",
        "print(np.sum(conv(data, filter2)))\n",
        "\n",
        "# Yes, this will always be the case because the sum of the filter itself is 0. Using\n",
        "# this filter, each of the values in the dataset will contribute positively at one\n",
        "# position with the 1 in the filter and a negative at one position with the -1. Then\n",
        "# 0 will not doing anything to the value. Therefore, with a positive, a negative,\n",
        "# and a neutral, the positive and negative will cancel each other out. This will\n",
        "# always be the case as long as there is the 0 padding on either side. Filters that\n",
        "# will produce this result are edge detectors, like [1, 0, -1], because it adds up\n",
        "# to 0. Filters, like these that the weights sum to 0, redistribute the given data\n",
        "# so that the positives and negatives exactly cancel each other out, producing that\n",
        "# 0 sum for the convolution."
      ],
      "metadata": {
        "id": "YHSIWjsJ5Txh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9607944d-2f77-4a99-f471-25dc1de39ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Discuss the following:\n",
        "* From the perspective of a human understanding the output of a convolution, are sum-preserving and a sum-zeroing filters useful?\n",
        "* In what sorts of circumstances might you wish to use them?\n"
      ],
      "metadata": {
        "id": "J2AF6sNn5U72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> From the perspective of a human understanding the output of a convolution, sum-preserving and sum-zeroing filters are both useful. Sum-preserving filters are typically used for smoothing out or blurring images without changing the brightness or intensity of the image. An example of a circumstance that you might use this in is with photography because a lot of the times you want to smooth out the image but keep the brightness and intensity of the overall image the same. This could be in regards to portraits where editors blur out blemishes and pores on the face but keep the brightness of the skin the same. Sum-zeroing filters are also useful because they are used for edge detection and emphasize contrasts in an imagee. An example of a circumstance where you might use this looking at medical images, like X-rays, and wanting to emphasize the contrast between the lines to detect important aspects of the X-rays to take note of, like fractures or tumors."
      ],
      "metadata": {
        "id": "n446AKwm9JEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Discuss the following:\n",
        "* For the purposes of machine computation in a neural network, are sum-preserving and sum-zeroing filters useful? Explain.\n",
        "* If they are usful, give an example of when one might be learned."
      ],
      "metadata": {
        "id": "EVtpBLsK6ybK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For the purpose of machine computation in a neural network, sum-preserving and sum-zeroing are useful, especially in CNN layers. Sum-preseving filters are useful for a neural network by smoothing and averaging information. It is also helpful in reducing any noise in the dataset, which helps the network focus in more on the general features. An example of when this might be learned is with smoothing out variations in handwriting styles, like with the MNIST dataset. Using sum-preserving filters for that dataset will maintain the structure of the dataset while also smoothing out those variations. Sum-zeroing filters are useful for a neural network because it is essential for emphasizing any changes in the dataset, like edges or gradients. An example of when this might be learned is in a CNN like YOLO, where these filters help the model identify the shapes of objects."
      ],
      "metadata": {
        "id": "iiGfmuc19IWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CIFAR\n",
        "\n",
        "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 70,000 color 32x32 images, in 10 different classes. Using PyTorch, build a CNN to classify the CIFAR-10 dataset. You can use the link above, in which case the data are already loaded into numpy arrays, but you may want to instead use [this repository](https://github.com/YoongiKim/CIFAR-10-images) or [this kaggle reupload](https://www.kaggle.com/datasets/swaroopkml/cifar10-pngs-in-folders), where they are image files.\n",
        "\n",
        "You should be able to achieve an accuracy >50% on this.\n",
        "\n",
        "Some hints:\n",
        "* This dataset is color images. We used black and white in clas.\n",
        "* Use batch normalization and a one-cycle fitting schedule to (hopefully) speed up the fitting. See the textbook.\n"
      ],
      "metadata": {
        "id": "f2NJkW3c6_9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *"
      ],
      "metadata": {
        "id": "JLbjC1_CGClo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env KAGGLE_USERNAME=sophia4827\n",
        "%env KAGGLE_KEY=ac334c3351a2af38b3f6b0ce9d9922a6\n",
        "\n",
        "!kaggle datasets download swaroopkml/cifar10-pngs-in-folders\n",
        "!unzip cifar10-pngs-in-folders.zip"
      ],
      "metadata": {
        "id": "GWYNHwCE-SWe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path(\".\")"
      ],
      "metadata": {
        "id": "2eqeNaHZH5j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock),\n",
        "                  get_items=get_image_files,\n",
        "                  splitter=RandomSplitter(seed=24601),\n",
        "                  get_y=parent_label)\n",
        "\n",
        "cifar_dls = cifar.dataloaders(path/'/content/cifar10/cifar10/train', bs=256)"
      ],
      "metadata": {
        "id": "qhvX-xM_HXG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_dls.show_batch(max_n=9, figsize=(4,4))"
      ],
      "metadata": {
        "id": "XBz-VweGILbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = sequential(\n",
        "    nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(8),\n",
        "    nn.Conv2d(8,16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.Conv2d(16,32, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Conv2d(32,64, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.Conv2d(64,128, kernel_size=3, stride=2, padding=1),\n",
        "    Flatten()\n",
        ")"
      ],
      "metadata": {
        "id": "GdDDWV0EIVyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(cifar_dls, cnn, loss_func=F.cross_entropy, metrics=accuracy)"
      ],
      "metadata": {
        "id": "fiqCIq1BIcB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(10, 1e-3)"
      ],
      "metadata": {
        "id": "gDwGjpwrIYSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Subword Tokenization\n",
        "\n",
        "**Note: This entire problem can be done in base Python. If you want to use a library, only add Pandas**.\n",
        "\n",
        "Subword tokenization follows this method:\n",
        "1. Begin with individual characters as your subword vocabulary. In English, if we make everything lowercase, then there should be just the letters \"a\" through \"z\" as the starting vocabulary\n",
        "2. Look at every possible two-token concatenation of the vocabulary, and see how many times each appears. Add the most common one to the vocabulary of subwords. You might find that \"th\" is the most common combination.\n",
        "3. Repeat step 2 until you hit the maximum size of your vocabulary.\n",
        "\n",
        "(a) The code below loads in the text of the novel *Pride and Prejudice* as plaintext, and then gives you a starting vocabulary of all the lowercase leters. Train a vocabulary on this text, with a maximum size of 500.\n",
        "\n",
        "(This can take a very long time if you're not clever about how you approach this. I'll give you partial credit if you only train to a vocabulary size of 100.)"
      ],
      "metadata": {
        "id": "B6iMw5dwKJZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "with urllib.request.urlopen('https://www.gutenberg.org/cache/epub/1342/pg1342.txt') as response:\n",
        "   text = response.read().decode(\"ascii\", \"ignore\")"
      ],
      "metadata": {
        "id": "6-a13uGCpAB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subwords = [chr(i) for i in range(97,97+26)]\n",
        "print(subwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sddmjV4-KQ8V",
        "outputId": "cca69f91-0eeb-4564-8670-abf4b2850dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for i in range(0,26):\n",
        "  for j in range(0,26):\n",
        "    two_letter = subwords[i] + subwords[j]\n",
        "    pairs.append(two_letter)"
      ],
      "metadata": {
        "id": "2qMFh02CcYXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_counts = {}\n",
        "\n",
        "for i in pairs:\n",
        "  count = text.count(i)\n",
        "\n",
        "  if count > 3000:\n",
        "    pair_counts[i] = count\n",
        "\n",
        "print(pair_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Hh-itObuC1",
        "outputId": "3691dc03-9851-4374-8dce-59d2b8e83b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'al': 3029, 'an': 8296, 'ar': 4439, 'as': 5015, 'at': 6177, 'be': 4038, 'co': 3028, 'ea': 3303, 'ed': 5708, 'en': 6681, 'er': 11978, 'es': 3977, 'ha': 6507, 'he': 15327, 'hi': 4644, 'in': 10618, 'is': 5183, 'it': 4954, 'le': 3803, 'li': 3220, 'll': 3452, 'me': 3294, 'nd': 5966, 'ne': 4130, 'ng': 5077, 'no': 3421, 'nt': 3658, 'of': 4298, 'on': 6593, 'or': 4302, 'ou': 6551, 're': 7820, 'se': 4042, 'st': 3921, 'te': 4855, 'th': 14078, 'ti': 3602, 'to': 5418, 've': 4499}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) What is the most common subword in the text? Explain why this is."
      ],
      "metadata": {
        "id": "6voAU6SFrGK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pair = max(pair_counts, key = pair_counts.get)\n",
        "max_count = pair_counts[max_pair]\n",
        "\n",
        "print(max_pair)\n",
        "print(max_count)\n",
        "\n",
        "# The most common subword in the text iss \"he\", which appears 15,327 times."
      ],
      "metadata": {
        "id": "IaOkahRGrcDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33779166-88cc-4d2c-fc68-cd532e4d6f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he\n",
            "15327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) What are the three longest subword in your vocabulary?"
      ],
      "metadata": {
        "id": "D4Xq00Bxrboy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "\n",
        "for i in range(0,26):\n",
        "  for j in range(0,26):\n",
        "    for k in range(0,26):\n",
        "      for l in range(0,26):\n",
        "        two_letter = subwords[i] + subwords[j] + subwords[k] + subwords[l]\n",
        "        pairs.append(two_letter)"
      ],
      "metadata": {
        "id": "P_5q87KergpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_counts = {}\n",
        "\n",
        "for i in pairs:\n",
        "  count = text.count(i)\n",
        "\n",
        "  if count > 1000:\n",
        "    pair_counts[i] = count\n",
        "\n",
        "print(pair_counts)\n",
        "\n",
        "# The three longest subwords in my vocabulary are \"tion\", \"ther\", and \"that\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IdzmUqkgmLk",
        "outputId": "7a57aafe-faa1-4084-bd35-8b2b28c1ed37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ould': 1280, 'that': 1609, 'ther': 1693, 'tion': 1744, 'with': 1342}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) What are the three most common subwords of at least 3 letters in length?"
      ],
      "metadata": {
        "id": "ID9t-jcTrgyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "\n",
        "for i in range(0,26):\n",
        "  for j in range(0,26):\n",
        "    for k in range(0,26):\n",
        "        two_letter = subwords[i] + subwords[j] + subwords[k]\n",
        "        pairs.append(two_letter)"
      ],
      "metadata": {
        "id": "XaY83bMFForp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_counts = {}\n",
        "\n",
        "for i in pairs:\n",
        "  count = text.count(i)\n",
        "\n",
        "  if count > 3000:\n",
        "    pair_counts[i] = count\n",
        "\n",
        "print(pair_counts)\n",
        "\n",
        "# The three most common subwords of at least 3 letters in length are \"the\", \"her\", and \"ing\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xd6nrUSg9iH",
        "outputId": "5c5675a8-52ad-4c2c-fa18-cf9fad331b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 3994, 'her': 4430, 'ing': 4163, 'the': 7870}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfU5OQQeqt3n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}